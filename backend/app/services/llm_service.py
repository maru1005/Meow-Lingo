# backend/app/services/llm_service.py
import os
import asyncio
import json
import logging
from openai import AsyncOpenAI
from .prompt_manager import prompt_manager # æ˜¨æ—¥ã®ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’æ´»ç”¨ï¼

# ğŸ’¡ ãƒ­ã‚°ã®è¨­å®šï¼šã“ã‚ŒãŒã‚ã‚Œã°ã€Œä½•ãŒèµ·ããŸã‹ã€ãŒé­”æ³•ã¿ãŸã„ã«ã‚ã‹ã‚‹ãƒ‹ãƒ£ï¼
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def get_ai_response(
        user_input: str, 
        dictionary_data: dict = None,
        messages_history: list | None = None,
        searchkeyword: str | None = None,
        ) -> str:
    """
    AIå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    
    # 1. PromptManagerã‚’ä½¿ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€
    # ğŸ’¡ æ˜¨æ—¥ã®åŠªåŠ›ï¼ˆPromptManagerï¼‰ã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãƒ‹ãƒ£ã€‚ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚‚ã‚ã£ã¡ã§ã‚„ã£ã¦ã‚‹ã‹ã‚‰ã‚¹ãƒƒã‚­ãƒªï¼
    base_prompt = prompt_manager.get_prompt("system_prompt.txt")
    
    full_system_prompt = f"""
{base_prompt}

ã€è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥è©³ç´°ãƒ«ãƒ¼ãƒ«ã€‘
- vocabulary: {prompt_manager.get_prompt("vocabulary.txt")}
- grammar: {prompt_manager.get_prompt("grammar.txt")}
- example: {prompt_manager.get_prompt("example.txt")}
- learning_advice: {prompt_manager.get_prompt("learning_advice.txt")}
- fallback: {prompt_manager.get_prompt("fallback.txt")}
"""

    # 2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®åˆæœŸåŒ–
    messages = [{"role": "system", "content": full_system_prompt}]

    # 3. éå»ä¼šè©±å±¥æ­´ã‚’ã‚»ãƒƒãƒˆ
    if messages_history:
        for msg in messages_history:
            messages.append({"role": msg.role, "content": msg.content})

    # 4. è¾æ›¸ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€æƒ…å ±ã‚’è¿½åŠ 
    if dictionary_data:
        dict_str = json.dumps(dictionary_data, ensure_ascii=False, indent=2)
        messages.append({
            "role": "system", 
            "content": f"### ã€æœ€å„ªå…ˆå‚ç…§ãƒ‡ãƒ¼ã‚¿ã€‘\nä»¥ä¸‹ã®è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã¯æœ€æ–°ã‹ã¤æ­£ç¢ºãªæƒ…å ±ã§ã™ã€‚ã‚ãªãŸã®çŸ¥è­˜ã‚ˆã‚Šã‚‚ã“ã®å†…å®¹ã‚’å„ªå…ˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„:\n{dict_str}"
        })

    # 5. ä»Šå›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’è¿½åŠ 
    messages.append({"role": "user", "content": user_input})

    # --- ğŸ’¡ ãƒ­ã‚°å‡ºåŠ›ï¼šã“ã“ãŒä»Šæ—¥å­¦ã‚“ã ã€Œè‰¯ã„ã“ã¨ã€ã®çµæ™¶ã ãƒ‹ãƒ£ï¼ ---
    logger.info(f"ğŸš€ [LLM Request] User Input: '{user_input[:20]}...' (Total messages: {len(messages)})")
    if dictionary_data:
        logger.info(f"ğŸ“– Dictionary data attached for: {dictionary_data.get('word', 'unknown')}")

    try:
        # 6. OpenAI APIå‘¼ã³å‡ºã—
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7
        )
        
        ai_content = response.choices[0].message.content
        logger.info("âœ… [LLM Response] Success! AI gave us an answer.")
        return ai_content

    except Exception as e:
        # ğŸ’¡ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚‚è©³ã—ãæ®‹ã›ã°ã€å¾Œã§ãƒªãƒ¼ãƒ€ãƒ¼ã‚’åŠ©ã‘ã¦ãã‚Œã‚‹ãƒ‹ãƒ£
        logger.error(f"âŒ [LLM Error] Something went wrong: {str(e)}", exc_info=True)
        return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚AIå¿œç­”ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰ï¼ˆçœç•¥ãªã—ï¼‰
if __name__ == "__main__":
    # ... (ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã¯ãã®ã¾ã¾)
    pass